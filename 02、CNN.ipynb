{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积层\n",
    "+ 卷积是将卷积核与输入进行卷积运算（图像处理一般称之为滤波），卷积核从左到右对输入进行扫描，每次滑动一格。\n",
    "+ 一般会使用多个卷积核对输入数据进行卷积，得到多个特征图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 激活层\n",
    "+ 因为卷积层进行的卷积计算是一种线性计算。\n",
    "+ 因此，需要对卷积层的输出进行一格非线性映射。一般使用relu激活函数（1、求导比tanh和sigmoid容易，减少计算量。2、使用tanh和sigmoid时，如果层数较多容易导致梯度消失。3、relu将小于0的映射为0，使得网络稀疏，减少神经元之间的依赖，避免过拟合。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 池化层\n",
    "+ 目的：减少特征图的维度，减少数据的运算量。\n",
    "+ 池化运算一般有两种 MaxPooling 和 MeanPooling。\n",
    "+ 计算方法： 选取一个池化窗口（一般是2*2），然后从左往右进行扫描，步长一般为2。 选取池化窗口中最大值作为该位置的输出（MaxPooling），或者选取平均值（MeanPooling）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全连接层\n",
    "+ 主要是对特征进行重新的拟合，减少特征信息的丢失。 \n",
    "+ 通过卷积池化操作后得到的是多个特征矩阵，而全连接层的输入为向量，所以在进行全连接层之前，要将多个特征矩阵“压平”为一个向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "用深度学习解决图像识别问题，从直观上讲是一个**从细节到抽象**的过程。所谓**细节，就是指输入图像的每个像素点，甚至像素点构成的边也可以理解为是细节**。假设我们大脑接收到一张动物图，大脑最先反应的是该图的点和边。然后由点和边抽象成各种形状，比如三角形或者圆形等，然后再抽象成耳朵和脸等特征。最后由这些特征决定该图属于哪种动物。深度学习识别图像也是同样的道理。这里关键的就是抽象。何为抽象呢？**抽象就是把图像中的各种零散的特征通过某种方式汇总起来，形成新的特征**。而利用这些新的特征可更好区分图像类别。如刚才这个例子，点和边就是零散的特征，通过将边进行汇总我们就得到了三角形或圆形等新的特征，同理，将三角形这个特征和一些其他零散的特征汇总成耳朵这个新特征。显而易见，耳朵这个新特征会比三角形特征更利于识别图像。\n",
    "\n",
    "深度学习正是**通过卷积操作实现从细节到抽象**的过程。因为**卷积的目的就是为了从输入图像中提取特征，并保留像素间的空间关系**。何以理解这句话？我们输入的图像其实就是一些纹理，此时，可以将卷积核的参数也理解为纹理，我们目的是使得卷积核的纹理和图像相应位置的纹理尽可能一致。当把图像数据和卷积核的数值放在高维空间中，纹理等价于向量，卷积操作等价于向量的相乘，相乘的结果越大，说明两个向量方向越近，也即卷积核的纹理就更贴近于图像的纹理。因此，卷积后的新图像在具有卷积核纹理的区域信号会更强，其他区域则会较弱。这样，就可以实现从细节（像素点）抽象成更好区分的新特征（纹理）。每一层的卷积都会得到比上一次卷积更易区分的新特征。\n",
    "\n",
    "而**池化目的主要就是为了减少权重参数**，但为什么可以以Maxpooling或者MeanPooling代表这个区域的特征呢？这样不会有可能损失了一些重要特征吗？这是因为**图像数据在连续区域具有相关性，一般局部区域的像素值差别不大**。比如眼睛的局部区域的像素点的值差别并不大，故我们使用Maxpooling或者MeanPooling并不会损失很多特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LeNet-5模型实例\n",
    "\n",
    "<img src='http://www.tensorflownews.com/wp-content/uploads/2018/03/LeNet-5.jpg' width=100% height=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一层：卷积层\n",
    "原始图像尺寸 = 32×32 × 1  \n",
    "卷积核 = 6 个 5×5  \n",
    "步长 = 1  \n",
    "不使用 0 填充  \n",
    "输出尺寸 = 32-5+1 = 28  \n",
    "输出深度 = 6 （6个卷积核）  \n",
    "\n",
    "## 第二层：池化层\n",
    "输入为第一层的输出 = 28×28 × 6 （经过 6 个 卷积核产生的 6 个 28×28 的layer）  \n",
    "过滤器 = 2×2  \n",
    "长、宽、步长 = 2  \n",
    "输出矩阵 = 14×14 × 6 （原来的 28×28 的层经过 2×2 的池化处理之后得到 14×14 ）  \n",
    "\n",
    "## 第三层：卷积层\n",
    "输入 = 14×14 × 6  \n",
    "卷积核 = 16 个 5×5  \n",
    "不使用 0 填充  \n",
    "步长 = 1  \n",
    "输出 = 10×10 × 16 （ 14-5+1 = 10 ）  \n",
    "\n",
    "## 第四层：池化层\n",
    "输出为上一层卷积层的输出  \n",
    "过滤器：2×2  \n",
    "步长 = 2  \n",
    "输出 = 5×5 × 16 （原来的 10×10 的层经过 2×2 的池化处理之后得到 5×5 ）  \n",
    "\n",
    "## 第五层：全连接层\n",
    "（在此之前，需要先将 5×5×16 的矩阵压扁为一个向量， 得到 160 个 输入神经元）  \n",
    "本层的输出节点个数 = 120  \n",
    "\n",
    "## 第六层：全连接层\n",
    "输入为上一层的输出  \n",
    "输出 = 84  \n",
    "\n",
    "## 第七层：全连接层\n",
    "输出 = 10 （样本标签的个数） （使用 one-hot encoding）  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "上述最终网络结构为  \n",
    "160 -- 120 -- 84 -- 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
